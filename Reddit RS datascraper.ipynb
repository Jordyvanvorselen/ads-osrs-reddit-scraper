{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "page = requests.get(\"https://www.reddit.com/r/2007scape/controversial/#res:ner-page=3\")\n",
    "page\n",
    "#page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#soup.find_all('p')[1].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for a in soup.find_all('a', href=True):\n",
    "#    print(\"Found the URL:\", a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ListOf\n",
    "#for a in soup.find_all('a', href=True):\n",
    "#    if \"71\" in a['href'] and \"https\" in a['href']:\n",
    "#        print(\"Found the URL:\", a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5 items i'll scrape information for:\n",
    "# 1: Bucket\n",
    "# 2: Trout\n",
    "# 3: Black kiteshield (g) (Or Black kite (g)/Black shield (g))\n",
    "# 4: Rune scimitar (Or Rune scimmy/scimmie)\n",
    "# 5: Studded body\n",
    "\n",
    "#Next page\n",
    "#https://www.reddit.com/r/2007scape/top/?sort=top&t=month\n",
    "#https://www.reddit.com/r/2007scape/top/?sort=top&t=month&count=25&after=t3_71a4g4\n",
    "#https://www.reddit.com/r/2007scape/top/?sort=top&t=month&count=50&after=t3_71zolr\n",
    "#nextPage = requests.get(\"https://www.reddit.com/r/2007scape/top/?sort=top&t=month\")\n",
    "#nextSoup = BeautifulSoup(nextPage.content, 'html.parser')\n",
    "#print(nextSoup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#last_id = nextSoup.find('div', attrs={'class': 'reportform report-t3_71a4g4'})\n",
    "#last_id = str(last_id)\n",
    "#last_id = last_id[30:]\n",
    "#last_id = last_id[:9]\n",
    "#next_link = \"https://www.reddit.com/r/2007scape/top/?sort=top&t=month&count=25&after=\" + last_id\n",
    "#next_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creates a list of 20 pages: ListOfPages\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def createListOfPages(numberOfPages):\n",
    "    currentPage = requests.get(\"https://www.reddit.com/r/2007scape/top/?sort=top&t=month\")\n",
    "    pageString = \"https://www.reddit.com/r/2007scape/top/?sort=top&t=month\"\n",
    "    ListOfPages = list()\n",
    "    pageCount = 0\n",
    "    while pageCount < numberOfPages:\n",
    "        while \"200\" not in str(currentPage):\n",
    "            currentPage = requests.get(pageString)\n",
    "        time.sleep(1)\n",
    "        Soup = BeautifulSoup(currentPage.content, 'html.parser')\n",
    "        ListOfPages.append(Soup)\n",
    "        pageCount = pageCount + 1\n",
    "        last_id = Soup.find('span', attrs={'class': 'next-button'})\n",
    "        pageString = last_id.find('a')['href']\n",
    "        print(str(pageString))\n",
    "        currentPage = requests.get(pageString)\n",
    "    print('ListOfPages created')\n",
    "    return ListOfPages\n",
    "        \n",
    "#print(last_id)\n",
    "#print(last_id['a href'])\n",
    "#last_id = Soup.findAll('div', attrs={'class': 'reportform report-t3_71a4g4'})\n",
    "#last_id = Soup.find('div', attrs={'class': 'reportform report-t3_71a4g4'})\n",
    "\n",
    "#last_id = str(last_id)\n",
    "#print(last_id)\n",
    "#last_id = last_id[119:]\n",
    "#last_id = last_id[:-39]\n",
    "#<span class=\"next-button\"><a href=\"https://www.reddit.com/r/2007scape/top/?sort=top&amp;t=month&amp;count=25&amp;after=t3_71a4g4\" rel=\"nofollow next\">next â€º</a></span>\n",
    "#last_id = last_id.find_all('a')\n",
    "#last_id = last_id['a']\n",
    "#print(str(last_id))\n",
    "\n",
    "#print(last_id)\n",
    "#pageString = \"https://www.reddit.com/r/2007scape/top/?sort=top&t=month&count=\" + str(pageCount * 25) + \"&after=\" + last_id  \n",
    "#print(pageString)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creates a list of topics ListOfTopics\n",
    "def createListOfTopicLinks(ListOfPages):\n",
    "    ListOfTopicLinks = list()\n",
    "    for page in ListOfPages:\n",
    "        #div class=\" thing id-t3_6yuv9t data-url\n",
    "        links = page.find_all('div', attrs={'class': 'thing'})\n",
    "        for link in links:\n",
    "            #print(link['data-url'])\n",
    "            ListOfTopicLinks.append(link['data-url'])\n",
    "    print('ListOfTopicLinks created')\n",
    "    return ListOfTopicLinks\n",
    "    \n",
    "#import re\n",
    "\n",
    "#for page in ListOfPages:\n",
    "#page = ListOfPages[0]\n",
    "#result = soup.find('a', attrs={'class': 'title may-blank outbound'})\n",
    "#print(result['data-href-url'])\n",
    "#currentPage = requests.get(result['data-href-url'])\n",
    "#while \"429\" in str(currentPage):\n",
    "#    currentPage = requests.get(result['data-href-url'])\n",
    "#currentSoup = BeautifulSoup(currentPage.content, 'html.parser')\n",
    "#textsFound = [m.start() for m in re.finditer('white_cat22', str(currentSoup))]\n",
    "#textsFound = str(currentSoup).findAll(\"white_cat22\")\n",
    "#counter = 0\n",
    "#for textFound in textsFound:\n",
    "#    print(str(counter) + \" Found:\")\n",
    "#    print(str(currentSoup)[(textFound - 50):(textFound + 50)])\n",
    "#    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creates DictionaryOfTitlesWithCommentLinks\n",
    "def createDictionaryOfTitlesWithCommentLinks(ListOfPages):\n",
    "    ListOfTopicTitles = list()\n",
    "    ListOfCommentLinks = list()\n",
    "    for page in ListOfPages:\n",
    "        #div class=\" thing id-t3_6yuv9t data-url\n",
    "        #print(page)\n",
    "        #title = page.find('a', attrs={'class': 'title'}).contents[0]\n",
    "        titlesSoup = page.find_all('a', attrs={'class': 'title'})\n",
    "        for soup in titlesSoup:\n",
    "            for title in soup.contents:\n",
    "                #print(title)\n",
    "                ListOfTopicTitles.append(title)\n",
    "        commentLinkSoup = page.find_all('a', attrs={'class': 'bylink comments may-blank'})\n",
    "        for commentSoup in commentLinkSoup:\n",
    "            #print(commentSoup.get('data-href-url'))\n",
    "            ListOfCommentLinks.append('https://www.reddit.com' + commentSoup.get('data-href-url'))\n",
    "        #print(idSoup)\n",
    "    print('DictionaryOfTitlesWithCommentLinks created')\n",
    "    return dict(zip(ListOfTopicTitles, ListOfCommentLinks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creates a list of comments ListOfComments\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "\n",
    "def createCommentData(word, soup):\n",
    "    #print(soup)\n",
    "    #div class=\"sitetable nestedlisting\"\n",
    "    #author = soup.find_all('div', attrs={'class': ' thing id-t1_'})\n",
    "    #author = author['data-author']\n",
    "    #print(author)\n",
    "    textsFound = [m.start() for m in re.finditer(word, str(soup))]\n",
    "    #print(textsFound)\n",
    "    result = list()\n",
    "    counter = 0\n",
    "    for textFound in textsFound:\n",
    "        result.append(str(soup)[(textFound - 100):(textFound + 100)])\n",
    "        counter = counter + 1\n",
    "    return commentData(author, result) \n",
    "\n",
    "def find_between(s, first, last):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "\n",
    "def findWordsInCommentPage(url, searchWords):\n",
    "    #print(\"opening: \" + url)\n",
    "    page = requests.get(url)\n",
    "    while \"200\" not in str(page):\n",
    "        page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    soup.html.find(text = True)\n",
    "    #print(soup.prettify())\n",
    "    #print('starting')\n",
    "    ListOfCommentDivs = soup.findAll('div', {'class' : 'entry'})#.find('data-author')\n",
    "    #print(len(ListOfCommentDivs))\n",
    "    #for div in ListOfCommentDivs:\n",
    "    ListOfCommentData = list()\n",
    "    return ListOfCommentData\n",
    "        #print('Author: ' + str(div.select('.tagline .author')) + '\\n')\n",
    "        #print('Timestamp: ' + str(div.select('.tagline .live-timestamp'))  + '\\n')\n",
    "        #print('Text: ' + str(div.select('.usertext-body')) + \"\\n\\n\")\n",
    "    #ListOfCommentDivs2 = soup.find('div', {'class' : 'entry'})#.findAll('div', {'class' : 'comment'})\n",
    "    #ListOfCommentDivs = soup.select('.entry .usertext')\n",
    "    #ListOfComments = list()\n",
    "    #for div in ListOfCommentDivs:\n",
    "    #    ListOfComments.append(find_between(div, '<p>', '</p>'))\n",
    "    #for comment in ListOfComments:\n",
    "    #    print(comment)\n",
    "    #print (soup)\n",
    "    #!!!! maak eerst een lijst van alle soups/comments en ga daar de info uit slopen, niet te doen zo\n",
    "    #for word in searchWords:\n",
    "    #    result = createCommentData(word, soup)\n",
    "    #    ListOfCommentData.append(result)\n",
    "    #print('ListOfCommentData created')\n",
    "    \n",
    "# 5 items i'll scrape information for:\n",
    "# 1: Bucket\n",
    "# 2: Trout\n",
    "# 3: Black kiteshield (g) (Or Black kite (g)/Black shield (g))\n",
    "# 4: Rune scimitar (Or Rune scimmy/scimmie)\n",
    "# 5: Studded body\n",
    "\n",
    "#def match_words(words, string):\n",
    "#re_words = '|'.join(map(re.escape, sorted(words, key=len, reverse=True)))\n",
    "#return re.search(r\"\\b(?:{words})\\b\".format(words=re_words), string)\n",
    "    \n",
    "#words = match_words(\"Amethyst\", str(soup))\n",
    "#print(words)\n",
    "#print(soup[words.span])\n",
    "#for word in words:\n",
    "#    print(\"hello\")\n",
    "#    print(word)\n",
    "#if words.count is 0:\n",
    "#    print(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, json, pprint\n",
    "\n",
    "class commentData():\n",
    "    def __init__(self, author, comment):\n",
    "        self.author = author\n",
    "        self.comment = comment\n",
    "    \n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    #def addComment(comment):\n",
    "    #    self.comments.append(comment)\n",
    "\n",
    "def createCommentFromReply(replyObj):\n",
    "    if str(replyObj['kind']) == 't1':\n",
    "        return commentData(replyObj['data']['author'], replyObj['data']['body'])\n",
    "        #print('Reply:')\n",
    "        #print(replyObj['data']['author'])\n",
    "        #print(replyObj['data']['body'] + '\\n\\n\\n')\n",
    "        \n",
    "\n",
    "def createCommentFromJson(commentObj):\n",
    "    ListOfComments = list()\n",
    "    if str(commentObj['kind']) == 't1':\n",
    "        comment = commentData(commentObj['data']['author'], commentObj['data']['body'])\n",
    "        #print('Comment:')\n",
    "        #print(commentObj['data']['author'])\n",
    "        #print(commentObj['data']['body'] + '\\n\\n\\n')\n",
    "        #for line in commentObj['data']['body']:\n",
    "        #    comment.addComment(line)\n",
    "        ListOfComments.append(comment)\n",
    "        if commentObj['data']['replies'] != '':\n",
    "            for replyObj in commentObj['data']['replies']['data']['children']:\n",
    "                ListOfComments.append(createCommentFromReply(replyObj))\n",
    "    return ListOfComments\n",
    "        #for reply in commentObj['data']['replies']:\n",
    "            #for replyObj in reply['data']['children']:\n",
    "            #    print('Reply:')\n",
    "            #    createCommentFromJson(replyObj)\n",
    "\n",
    "def getCommentsWithAuthorAndDate(url):\n",
    "    ListOfCommentsWithAuthorAndDate = list()\n",
    "    \n",
    "    pp = pprint.PrettyPrinter(indent=2)\n",
    "    response = requests.get(url, headers={'user-agent': 'Mozilla/5.0'})\n",
    "    page_json = json.loads(response.text)\n",
    "    \n",
    "    for obj in page_json:\n",
    "        ListOfComments = list()\n",
    "        #print(pp.pprint(obj['data']['children']))\n",
    "        for commentObj in obj['data']['children']:\n",
    "            comments = createCommentFromJson(commentObj)\n",
    "            for comment in comments:\n",
    "                ListOfComments.append(comment)\n",
    "    for comment in ListOfComments:\n",
    "        ListOfCommentsWithAuthorAndDate.append(comment)\n",
    "    return ListOfCommentsWithAuthorAndDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/2007scape/top/?sort=top&t=month&count=25&after=t3_71q372\n",
      "ListOfPages created\n",
      "ListOfTopicLinks created\n",
      "DictionaryOfTitlesWithCommentLinks created\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Execute all code and write to txt\n",
    "ListOfPages = createListOfPages(1)\n",
    "ListOfTopicLinks = createListOfTopicLinks(ListOfPages)\n",
    "DictionaryOfTitlesWithCommentLinks = createDictionaryOfTitlesWithCommentLinks(ListOfPages)\n",
    "#searchWords = [\"black\", \"bronze\", \"iron\", \"dragon\", \"money\"]\n",
    "for key, value in DictionaryOfTitlesWithCommentLinks.items():\n",
    "#    print(key + \"|\" + value)\n",
    "    ListOfCommentsWithAuthorAndDate = getCommentsWithAuthorAndDate(value + '.json')\n",
    "    #foundStrings = findWordsInCommentPage(value, searchWords)\n",
    "    \n",
    "print('Done')\n",
    "    \n",
    "#searchWords = [\"runescape\"] \n",
    "#ListOfCommentData = findWordsInCommentPage('https://www.reddit.com/r/2007scape/comments/6yuv9t/white_cat22/', searchWords)\n",
    "\n",
    "\n",
    "\n",
    "#import json\n",
    "\n",
    "#with open('CommentsData.txt', 'w') as outfile:  \n",
    "#    json.dump(ListOfCommentData, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import requests\n",
    "#from bs4 import BeautifulSoup\n",
    "\n",
    "#page = requests.get(\"https://www.reddit.com/r/2007scape/comments/6yuv9t/white_cat22/\")\n",
    "#while \"200\" not in str(page):\n",
    "#    page = requests.get(\"https://www.reddit.com/r/2007scape/comments/6yuv9t/white_cat22/\")\n",
    "#soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "#print(soup.prettify)\n",
    "#li = soup.find('div', attrs={'class': ' thing id-t1_'})\n",
    "#li\n",
    "#pageString = last_id.find('a')['href']\n",
    "#children = li.findChildren()\n",
    "#for child in children:\n",
    "#    print (child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to file\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "file = open(\"D:\\Minor ADS\\Large files\\FoundRedditStrings.txt\",\"w\")\n",
    "print('Writing to file')\n",
    "print(len(ListOfCommentsWithAuthorAndDate))\n",
    "for comment in ListOfCommentsWithAuthorAndDate:\n",
    "    file.write(comment.toJSON())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
